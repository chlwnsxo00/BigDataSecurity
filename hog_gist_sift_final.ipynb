{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.util import view_as_blocks\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabor 필터 생성 함수\n",
    "def create_filters(scales, orientations):\n",
    "    filters = []\n",
    "    for scale in range(scales[0], scales[1] + 1):\n",
    "        for orientation in np.arange(0, np.pi, np.pi / orientations):\n",
    "            filt_real = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0, ktype=cv2.CV_32F)\n",
    "            filt_imag = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0.5 * np.pi, ktype=cv2.CV_32F)\n",
    "            filt = filt_real + filt_imag\n",
    "            filt /= 2.0 * np.pi * scale * scale\n",
    "            filters.append(filt)\n",
    "    return filters\n",
    "\n",
    "# HOG 디스크립터 계산 함수\n",
    "def hog_descriptor_single_channel(image, scales=(8, 8), orientations=8, blocks=(4, 4)):    # Gabor 필터 생성\n",
    "    filters = create_filters(scales, orientations)\n",
    "    \n",
    "    # 이미지 크기와 블록 크기 계산\n",
    "    height, width = image.shape[:2]\n",
    "    block_size = height // blocks[0], width // blocks[1]\n",
    "\n",
    "    padding_size = blocks[0] * block_size[0] - height, blocks[1] * block_size[1] - width\n",
    "    \n",
    "    # 이미지 패딩 (필요한 경우)\n",
    "    if padding_size != (0, 0):\n",
    "        image = cv2.copyMakeBorder(image, 0, padding_size[0], 0, padding_size[1], cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    # 이미지를 블록으로 분할\n",
    "    block_shape = (block_size[0], block_size[1])\n",
    "    blocks = view_as_blocks(image, block_shape=(block_size[0], block_size[1])).reshape(-1, *block_size, order='F')\n",
    "    \n",
    "    # 각 블록의 hog 특성 추출\n",
    "    features = []\n",
    "    for block in blocks:\n",
    "        feats = []\n",
    "        for scale in filters:\n",
    "            for filt in scale:\n",
    "                filtered = cv2.filter2D(block, cv2.CV_64F, filt)\n",
    "                feats.append(filtered.mean())\n",
    "        features.append(feats)\n",
    "    \n",
    "    # 전체 hog 디스크립터로 결합\n",
    "    return np.concatenate(features)\n",
    "\n",
    "\n",
    "def hog_descriptor(image, scales=(8, 8), orientations=8, blocks=(4, 4)):\n",
    "    if len(image.shape) == 3:\n",
    "    # 각 채널에 대해 hog 디스크립터 계산\n",
    "        descriptors = [hog_descriptor_single_channel(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), scales, orientations, blocks)]            # 전체 GIST 디스크립터로 결합\n",
    "        return np.concatenate(descriptors)\n",
    "    else:\n",
    "    # 단일 채널 이미지의 경우 hog 디스크립터를 한 번만 계산\n",
    "        return hog_descriptor_single_channel(image, scales, orientations, blocks)\n",
    "\n",
    "# 각 이미지에 맞는 레이블 생성\n",
    "def load_labels(dir, num_samples):\n",
    "    y = []\n",
    "    for subdir in sorted(os.listdir(dir)):\n",
    "        subdir_path = os.path.join(dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for i, filename in enumerate(sorted(os.listdir(subdir_path))):\n",
    "                y.append(labels_dict[subdir])\n",
    "                if len(y) == num_samples:\n",
    "                    break\n",
    "        if len(y) == num_samples:\n",
    "            break\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 악성코드 이미지 폴더에서 350개의 이미지에 대한 hog descriptor를 계산하여 반환\n",
    "def get_hog_descriptors(train_data):\n",
    "    descriptors = []\n",
    "    labels = []\n",
    "    for i, v in enumerate(train_data):\n",
    "      _,label,path = v\n",
    "      # 파일 경로 생성\n",
    "      # 이미지 로드\n",
    "      image = cv2.imread(path)\n",
    "      # 이미지에 대한 hog 디스크립터 계산\n",
    "      descriptor = hog_descriptor(image)\n",
    "      descriptors.append(descriptor)\n",
    "      labels.append(label)\n",
    "      if i % 100 == 99:\n",
    "        print(\"\\tProcessed\", i + 1, \"images\")\n",
    "    return np.array(descriptors) , np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors, train_labels = get_hog_descriptors(train_data)\n",
    "print(len(descriptors))\n",
    "print('HOG Descriptor Shape:', descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 레이블 설정\n",
    "X = descriptors\n",
    "hog_features = X\n",
    "\n",
    "# hog_descriptors 및 레이블 불러오기\n",
    "X_train = X\n",
    "X_val , val_label= get_hog_descriptors(add_data)\n",
    "X_test, test_label = get_hog_descriptors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms1 = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),        \n",
    "        transforms.Normalize((0.37306938, 0.3730843, 0.3727943), (0.07064196, 0.0706386, 0.07048721))\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation((-20, 20)),\n",
    "        transforms.RandomGrayscale(p=0.8),\n",
    "        transforms.RandomHorizontalFlip(p=0.8),\n",
    "        transforms.RandomVerticalFlip(p=0.8),\n",
    "        transforms.ToTensor(),    \n",
    "        transforms.Normalize((0.37306938, 0.3730843, 0.3727943), (0.07064196, 0.0706386, 0.07048721))   \n",
    "    ])\n",
    "}\n",
    "\n",
    "image_transforms2 = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation((-20, 20)),\n",
    "        transforms.RandomGrayscale(p=0.8),\n",
    "        transforms.RandomHorizontalFlip(p=0.8),\n",
    "        transforms.RandomVerticalFlip(p=0.8),\n",
    "        transforms.ToTensor(),        \n",
    "        transforms.Normalize((0.37306938, 0.3730843, 0.3727943), (0.07064196, 0.0706386, 0.07048721))\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.37306938, 0.3730843, 0.3727943), (0.07064196, 0.0706386, 0.07048721))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = CustomDataset(root = train_path1, malware_list=malware_list,\n",
    "                                  transform = image_transforms1['train'])\n",
    "train_data2 = CustomDataset(root = train_path2, malware_list=malware_list,\n",
    "                                  transform = image_transforms2['train'])\n",
    "\n",
    "test_data1 = CustomDataset(root = test_path1, malware_list=malware_list,\n",
    "                                  transform = image_transforms1['test'])\n",
    "test_data2 = CustomDataset(root = test_path2, malware_list=malware_list,\n",
    "                                  transform = image_transforms2['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learing result of HOG descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Define scoring functions\n",
    "scores = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1': f1_score,\n",
    "    'Confusion Matrix': confusion_matrix\n",
    "}\n",
    "\n",
    "# Model definitions\n",
    "models = [\n",
    "    ('Random Forest', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100))),\n",
    "    ('XGBoost', XGBClassifier(learning_rate=0.01, reg_lambda=0.1)),\n",
    "    ('Linear SVM', make_pipeline(StandardScaler(), LinearSVC(penalty='l2', dual=False))),\n",
    "    ('SMO', make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale', probability=True))),\n",
    "    ('J48', DecisionTreeClassifier())\n",
    "]\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = ['Malware', 'Benign']\n",
    "# Ensemble model definition\n",
    "ensemble_model = VotingClassifier(models, voting='hard')\n",
    "\n",
    "# Cross-validation and evaluation for each model\n",
    "for model_name, model_instance in models:\n",
    "    print(model_name)\n",
    "    model_instance.fit(X_train, train_labels)\n",
    "    y_pred = model_instance.predict(X_test)\n",
    "    for score_name, score_func in scores.items():\n",
    "        # if score_name == 'Confusion Matrix':\n",
    "        #     cm = score_func(test_label, y_pred)\n",
    "        #     print(score_name)\n",
    "        #     print(cm)\n",
    "        #     plot_confusion_matrix(cm, class_names, model_name)\n",
    "\n",
    "        # else:\n",
    "            print(score_name, score_func(test_label, y_pred))\n",
    "\n",
    "# Ensemble model training and evaluation\n",
    "ensemble_model.fit(X_train, train_labels)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ensemble Model')\n",
    "for score_name, score_func in scores.items():\n",
    "    if score_name == 'Confusion Matrix':\n",
    "        cm = score_func(test_label, y_pred_ensemble)\n",
    "        print(score_name)\n",
    "        print(cm)\n",
    "        plot_confusion_matrix(cm, class_names, 'Ensemble Model')\n",
    "\n",
    "    else:\n",
    "        print(score_name, score_func(test_label, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = ['Malware', 'Benign']\n",
    "\n",
    "print('Ensemble Model')\n",
    "for score_name, score_func in scores.items():\n",
    "    if score_name == 'Confusion Matrix':\n",
    "        cm = score_func(test_label, y_pred_ensemble)\n",
    "        print(score_name)\n",
    "        print(cm)\n",
    "        plot_confusion_matrix(cm, class_names, 'Ensemble Model')\n",
    "    else:\n",
    "        print(score_name, score_func(test_label, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation acurracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\n",
    "    ('XGBoost', xgb.XGBClassifier()),\n",
    "    ('Linear SVM', LinearSVC()),\n",
    "    ('SMO', SVC(kernel='poly', coef0=1.0, C=1.0, degree=3)),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('J48', DecisionTreeClassifier(max_depth=6))\n",
    "]\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def run_cross_validation(X_train, train_labels, models, n_splits=5):\n",
    "    scores = {model_name: {'Validation Accuracy': [], 'Validation Loss': []} for model_name, _ in models}\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits = n_splits)\n",
    "    for train_index, val_index in cv.split(X_train, train_labels):\n",
    "        X_cv_train, X_cv_val = X_train[train_index], X_train[val_index]\n",
    "        y_cv_train, y_cv_val = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "        for model_name, model_instance in models:\n",
    "            print(f\"Processing {model_name}...\")\n",
    "            sc = StandardScaler()\n",
    "            X_cv_train_std = sc.fit_transform(X_cv_train)\n",
    "            model = model_instance\n",
    "            model.fit(X_cv_train_std, y_cv_train)\n",
    "    \n",
    "            X_cv_val_std = sc.transform(X_cv_val)\n",
    "            y_val_pred = model.predict(X_cv_val_std)\n",
    "            scores[model_name]['Validation Accuracy'].append(accuracy_score(y_cv_val, y_val_pred))\n",
    "            scores[model_name]['Validation Loss'].append(root_mean_squared_error(y_cv_val, y_val_pred))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores_per_model = run_cross_validation(X_train, train_labels, models)\n",
    "\n",
    "# Plot validation accuracy per model\n",
    "n_splits = 5  # or any positive integer value\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "for model_name, model_scores in scores_per_model.items():\n",
    "    plt.plot(range(1, n_splits + 1), model_scores['Validation Accuracy'], label=model_name)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation loss per model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "for model_name, model_scores in scores_per_model.items():\n",
    "    plt.plot(range(1, n_splits + 1), model_scores['Validation Loss'], label=model_name)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.util import view_as_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabor 필터 생성 함수\n",
    "def create_filters(scales, orientations):\n",
    "    filters = []\n",
    "    for scale in range(scales[0], scales[1] + 1):\n",
    "        for orientation in np.arange(0, np.pi, np.pi / orientations):\n",
    "            filt_real = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0, ktype=cv2.CV_32F)\n",
    "            filt_imag = cv2.getGaborKernel((scale, scale), 1, orientation, scale, 0.5 * np.pi, ktype=cv2.CV_32F)\n",
    "            filt = filt_real + filt_imag\n",
    "            filt /= 2.0 * np.pi * scale * scale\n",
    "            filters.append(filt)\n",
    "    return filters\n",
    "\n",
    "# GIST 디스크립터 계산 함수\n",
    "def gist_descriptor_single_channel(image, scales=(8, 8), orientations=8, blocks=(4, 4)):    # Gabor 필터 생성\n",
    "    filters = create_filters(scales, orientations)\n",
    "    \n",
    "    # 이미지 크기와 블록 크기 계산\n",
    "    height, width = image.shape[:2]\n",
    "    block_size = height // blocks[0], width // blocks[1]\n",
    "\n",
    "    padding_size = blocks[0] * block_size[0] - height, blocks[1] * block_size[1] - width\n",
    "    \n",
    "    # 이미지 패딩 (필요한 경우)\n",
    "    if padding_size != (0, 0):\n",
    "        image = cv2.copyMakeBorder(image, 0, padding_size[0], 0, padding_size[1], cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    # 이미지를 블록으로 분할\n",
    "    block_shape = (block_size[0], block_size[1])\n",
    "    blocks = view_as_blocks(image, block_shape=(block_size[0], block_size[1])).reshape(-1, *block_size, order='F')\n",
    "    \n",
    "    # 각 블록의 GIST 특성 추출\n",
    "    features = []\n",
    "    for block in blocks:\n",
    "        feats = []\n",
    "        for scale in filters:\n",
    "            for filt in scale:\n",
    "                filtered = cv2.filter2D(block, cv2.CV_64F, filt)\n",
    "                feats.append(filtered.mean())\n",
    "        features.append(feats)\n",
    "    \n",
    "    # 전체 GIST 디스크립터로 결합\n",
    "    return np.concatenate(features)\n",
    "\n",
    "\n",
    "def gist_descriptor(image, scales=(8, 8), orientations=8, blocks=(4, 4)):\n",
    "    if len(image.shape) == 3:\n",
    "    # 각 채널에 대해 GIST 디스크립터 계산\n",
    "        descriptors = [gist_descriptor_single_channel(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), scales, orientations, blocks)]            # 전체 GIST 디스크립터로 결합\n",
    "        return np.concatenate(descriptors)\n",
    "    else:\n",
    "    # 단일 채널 이미지의 경우 GIST 디스크립터를 한 번만 계산\n",
    "        return gist_descriptor_single_channel(image, scales, orientations, blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 악성코드 이미지 폴더에서 350개의 이미지에 대한 gist descriptor를 계산하여 반환\n",
    "def get_gist_descriptors(root_dir):\n",
    "    descriptors = []\n",
    "    for subdir in sorted(os.listdir(root_dir)):\n",
    "        subdir_path = os.path.join(root_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            print(\"Processing directory:\", subdir_path)\n",
    "            for i, filename in enumerate(os.listdir(subdir_path)):\n",
    "                # 파일 경로 생성\n",
    "                filepath = os.path.join(subdir_path, filename)\n",
    "                # 이미지 로드\n",
    "                image = cv2.imread(filepath)\n",
    "                # 이미지에 대한 GIST 디스크립터 계산\n",
    "                descriptor = gist_descriptor(image)\n",
    "                descriptors.append(descriptor)\n",
    "                \n",
    "                if i % 10 == 9:\n",
    "                    print(\"\\tProcessed\", i + 1, \"images\")\n",
    "    return np.array(descriptors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, width, height):\n",
    "    return cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def get_GIST_descriptors(train_data, img_width=64, img_height=64):\n",
    "    gist_descriptors = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i, v in enumerate(train_data):\n",
    "        _, label, path = v\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # 이미지 크기 조정\n",
    "        img = resize_image(img, img_width, img_height)\n",
    "\n",
    "        gist_descriptor = gist(img, orientations=8, pixels_per_cell=(8, 8),\n",
    "                             cells_per_block=(1, 1), block_norm='L2-Hys')\n",
    "        gist_descriptors.append(gist_descriptor)\n",
    "        train_labels.append(label)\n",
    "\n",
    "        # 진행 상황 출력\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1} images\")\n",
    "    \n",
    "    gist_descriptors = np.array(gist_descriptors)    \n",
    "    return gist_descriptors, train_labels\n",
    "\n",
    "# Assuming 'train_data' is a list of (_, label, path) tuples\n",
    "h_descriptors, train_labels = get_GIST_descriptors(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 레이블 설정\n",
    "X = h_descriptors\n",
    "gist_features = X\n",
    "\n",
    "# gist_descriptors 및 레이블 불러오기\n",
    "X_train = X\n",
    "X_val , val_label= get_GIST_descriptors(add_data)\n",
    "X_test, test_label = get_GIST_descriptors(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machin learning and model ensemble result of GIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Define scoring functions\n",
    "scores = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1': f1_score,\n",
    "    'Confusion Matrix': confusion_matrix\n",
    "}\n",
    "\n",
    "# Model definitions\n",
    "models = [\n",
    "    ('Random Forest', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100))),\n",
    "    ('XGBoost', XGBClassifier(learning_rate=0.01, reg_lambda=0.1)),\n",
    "    ('Linear SVM', make_pipeline(StandardScaler(), LinearSVC(penalty='l2', dual=False))),\n",
    "    ('SMO', make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale', probability=True))),\n",
    "    ('J48', DecisionTreeClassifier())\n",
    "]\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = ['Malware', 'Benign']\n",
    "# Ensemble model definition\n",
    "ensemble_model = VotingClassifier(models, voting='hard')\n",
    "\n",
    "# Cross-validation and evaluation for each model\n",
    "for model_name, model_instance in models:\n",
    "    print(model_name)\n",
    "    model_instance.fit(X_train, train_labels)\n",
    "    y_pred = model_instance.predict(X_test)\n",
    "    for score_name, score_func in scores.items():\n",
    "        # if score_name == 'Confusion Matrix':\n",
    "        #     cm = score_func(test_label, y_pred)\n",
    "        #     print(score_name)\n",
    "        #     print(cm)\n",
    "        #     plot_confusion_matrix(cm, class_names, model_name)\n",
    "\n",
    "        # else:\n",
    "            print(score_name, score_func(test_label, y_pred))\n",
    "\n",
    "# Ensemble model training and evaluation\n",
    "ensemble_model.fit(X_train, train_labels)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = ['Malware', 'Benign']\n",
    "\n",
    "print('Ensemble Model')\n",
    "for score_name, score_func in scores.items():\n",
    "    if score_name == 'Confusion Matrix':\n",
    "        cm = score_func(test_label, y_pred_ensemble)\n",
    "        print(score_name)\n",
    "        print(cm)\n",
    "        plot_confusion_matrix(cm, class_names, 'Ensemble Model')\n",
    "    else:\n",
    "        print(score_name, score_func(test_label, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\n",
    "    ('XGBoost', xgb.XGBClassifier()),\n",
    "    ('Linear SVM', LinearSVC()),\n",
    "    ('SMO', SVC(kernel='poly', coef0=1.0, C=1.0, degree=3)),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('J48', DecisionTreeClassifier(max_depth=6))\n",
    "]\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def run_cross_validation(X_train, train_labels, models, n_splits=5):\n",
    "    scores = {model_name: {'Validation Accuracy': [], 'Validation Loss': []} for model_name, _ in models}\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits = n_splits)\n",
    "    for train_index, val_index in cv.split(X_train, train_labels):\n",
    "        X_cv_train, X_cv_val = X_train[train_index], X_train[val_index]\n",
    "        train_labels = np.array(train_labels)  # 이 줄을 추가하여 train_labels를 numpy 배열로 변환합니다.\n",
    "        y_cv_train, y_cv_val = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "        for model_name, model_instance in models:\n",
    "            print(f\"Processing {model_name}...\")\n",
    "            sc = StandardScaler()\n",
    "            X_cv_train_std = sc.fit_transform(X_cv_train)\n",
    "            model = model_instance\n",
    "            model.fit(X_cv_train_std, y_cv_train)\n",
    "    \n",
    "            X_cv_val_std = sc.transform(X_cv_val)\n",
    "            y_val_pred = model.predict(X_cv_val_std)\n",
    "            scores[model_name]['Validation Accuracy'].append(accuracy_score(y_cv_val, y_val_pred))\n",
    "            scores[model_name]['Validation Loss'].append(root_mean_squared_error(y_cv_val, y_val_pred))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores_per_model = run_cross_validation(X_train, train_labels, models)\n",
    "\n",
    "# Plot validation accuracy per model\n",
    "n_splits = 5  # or any positive integer value\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "for model_name, model_scores in scores_per_model.items():\n",
    "    plt.plot(range(1, n_splits + 1), model_scores['Validation Accuracy'], label=model_name)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation loss per model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "for model_name, model_scores in scores_per_model.items():\n",
    "    plt.plot(range(1, n_splits + 1), model_scores['Validation Loss'], label=model_name)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_features = []\n",
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_sift_descriptors(train_data):\n",
    "    sift = cv2.SIFT_create()\n",
    "    sift_descriptors = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i, v in enumerate(train_data):\n",
    "        _, label, path = v\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        _, descriptor = sift.detectAndCompute(img, None)\n",
    "\n",
    "        # If no feature is detected, skip this image\n",
    "        if descriptor is None:\n",
    "            continue\n",
    "\n",
    "        sift_descriptors.append(descriptor)\n",
    "        train_labels.append(label)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1} images\")\n",
    "\n",
    "    # Convert the list of descriptors to an ndarray\n",
    "    # and zero-padding to match image with fewer keypoints\n",
    "    max_keypoints = max([desc.shape[0] for desc in sift_descriptors])\n",
    "    s_descriptors = np.zeros((len(sift_descriptors), max_keypoints, 128))\n",
    "    for i, desc in enumerate(sift_descriptors):\n",
    "        s_descriptors[i, :desc.shape[0], :] = desc\n",
    "\n",
    "    return s_descriptors, train_labels\n",
    "\n",
    "# Assuming 'train_data' is a list of (_, label, path) tuples\n",
    "s_descriptors, train_labels = get_sift_descriptors(train_data)\n",
    "print(len(s_descriptors))\n",
    "print('SIFT Descriptor Shape:', s_descriptors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 레이블 설정\n",
    "X = s_descriptors\n",
    "\n",
    "# hog_descriptors 및 레이블 불러오기\n",
    "X_train = X\n",
    "X_val , val_label= get_sift_descriptors(add_data)\n",
    "X_test, test_label = get_sift_descriptors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming s_descriptors is the 3D SIFT descriptors array with shape (num_samples, num_keypoints, num_features)\n",
    "\n",
    "# Step 1: Flatten SIFT descriptors from 3D to 2D\n",
    "s_descriptors_flat = s_descriptors.reshape(s_descriptors.shape[0], -1)\n",
    "\n",
    "# Step 2: Apply StandardScaler to the flattened 2D array\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(s_descriptors_flat)\n",
    "\n",
    "# Now, train the model with the transformed data\n",
    "model_instance.fit(X_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machin Learning result abd model ensemble of SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Define scoring functions\n",
    "scores = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1': f1_score,\n",
    "    'Confusion Matrix': confusion_matrix\n",
    "}\n",
    "\n",
    "# Model definitions\n",
    "models = [\n",
    "    ('Random Forest', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100))),\n",
    "    ('XGBoost', XGBClassifier(learning_rate=0.01, reg_lambda=0.1)),\n",
    "    ('Linear SVM', make_pipeline(StandardScaler(), LinearSVC(penalty='l2', dual=False))),\n",
    "    ('SMO', make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale', probability=True))),\n",
    "    ('J48', DecisionTreeClassifier())\n",
    "]\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = ['Malware', 'Benign']\n",
    "# Ensemble model definition\n",
    "ensemble_model = VotingClassifier(models, voting='hard')\n",
    "\n",
    "# Cross-validation and evaluation for each model\n",
    "for model_name, model_instance in models:\n",
    "    print(model_name)\n",
    "    model_instance.fit(X_train, train_labels)\n",
    "    y_pred = model_instance.predict(X_test)\n",
    "    for score_name, score_func in scores.items():\n",
    "        # if score_name == 'Confusion Matrix':\n",
    "        #     cm = score_func(test_label, y_pred)\n",
    "        #     print(score_name)\n",
    "        #     print(cm)\n",
    "        #     plot_confusion_matrix(cm, class_names, model_name)\n",
    "\n",
    "        # else:\n",
    "            print(score_name, score_func(test_label, y_pred))\n",
    "\n",
    "# Ensemble model training and evaluation\n",
    "ensemble_model.fit(X_train, train_labels)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
